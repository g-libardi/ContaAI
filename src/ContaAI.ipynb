{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RxXWKQX4jDg"
      },
      "source": [
        "# Download e tratamento de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43IKg6WtIivi"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOgcpR_V8b0S",
        "outputId": "db54964b-0e56-472b-de83-5d5e05eeb2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkqn0J29Khab"
      },
      "outputs": [],
      "source": [
        "def obter_diretorios(site_url):\n",
        "    # Faz uma solicitação GET ao site\n",
        "    response = requests.get(site_url)\n",
        "\n",
        "    # Verifica se a solicitação foi bem-sucedida\n",
        "    if response.status_code == 200:\n",
        "        padrao = r\"https://visionvox.net/biblioteca/[a-z]/.+\\.txt\"\n",
        "        matches = re.findall(padrao, response.text)\n",
        "        return [match.split('>')[0] for match in matches]\n",
        "    else:\n",
        "        print(\"Falha ao obter a página:\", response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu5JrY4LPLmz"
      },
      "outputs": [],
      "source": [
        "with open('diretorios.txt', 'w') as f:\n",
        "  for i in range(600):\n",
        "    try:\n",
        "      url = f'https://visionvox.com.br/biblioteca/estante.php?estante=formato&formato=txt&pagina=sim&num_page={i*50}&total_pagina=2097.58'\n",
        "      dirs = obter_diretorios(url)\n",
        "      for dir in dirs:\n",
        "        f.write(dir + '\\n')\n",
        "    except:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YnqmknMkWapi"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/IA/diretorios.txt', 'r') as f:\n",
        "  links = [x.strip() for x in f.readlines()]\n",
        "\n",
        "n = 1000\n",
        "random.seed(47)\n",
        "links = random.sample(links, n)\n",
        "\n",
        "for i, link in enumerate(links):\n",
        "  if i >= 500:\n",
        "    with open(f'/content/drive/MyDrive/IA/books/book{i:05}.txt', 'w') as f:\n",
        "      response = requests.get(link)\n",
        "      f.write(response.text + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3bkLmbtt0ucG"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/IA/data.txt', 'w') as out:\n",
        "  for i in range(1000):\n",
        "    with open(f'/content/drive/MyDrive/IA/books/book{i:05}.txt', 'r') as f:\n",
        "      content = f.read()\n",
        "    out.write(content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw_Dxggfk8-J",
        "outputId": "4a7906b2-fae0-4743-a455-bb8fc71450bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data length: 4651204\n",
            "Train length: 3953523\n",
            "Test length: 697681\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('/content/drive/MyDrive/IA/data.txt', 'r') as f:\n",
        "  data = f.readlines()\n",
        "\n",
        "train, test = train_test_split(data,test_size=0.15)\n",
        "\n",
        "print(\"Data length: \" + str(len(data)))\n",
        "print(\"Train length: \" + str(len(train)))\n",
        "print(\"Test length: \" + str(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "foAygkjFlH_p"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/IA/train.txt', 'w') as f:\n",
        "  for line in train:\n",
        "    f.write(line)\n",
        "with open('/content/drive/MyDrive/IA/test.txt', 'w') as f:\n",
        "  for line in test:\n",
        "    f.write(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EFrIWLj4zg6"
      },
      "source": [
        "# Preparando modelo, tokenizer e dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcaVB73p48km",
        "outputId": "798fe9cf-3150-445d-8581-46c881b8511f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from evaluate)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tokenizers, safetensors, xxhash, mypy-extensions, dill, typing-inspect, responses, multiprocess, huggingface-hub, transformers, pyre-extensions, datasets, evaluate, xformers, accelerate\n",
            "Successfully installed accelerate-0.20.3 datasets-2.13.1 dill-0.3.6 evaluate-0.4.0 huggingface-hub-0.15.1 multiprocess-0.70.14 mypy-extensions-1.0.0 pyre-extensions-0.0.29 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 typing-inspect-0.9.0 xformers-0.0.20 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate xformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_84kFqkw5Oah"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaCUEqsu6JpY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('pierreguillou/gpt2-small-portuguese')\n",
        "\n",
        "train_path = '/content/drive/MyDrive/IA/train.txt'\n",
        "test_path = '/content/drive/MyDrive/IA/test.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GZ34dSl8Adg"
      },
      "outputs": [],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=1)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmRDYq15HowS"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/IA/gpt2-contaAI-small-1/\", # The output directory\n",
        "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
        "    num_train_epochs=2, # number of training epochs\n",
        "    per_device_train_batch_size=96, # batch size for training\n",
        "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
        "    eval_steps = 500, # Number of update steps between two evaluations.\n",
        "    save_steps=1000, # after # steps model is saved\n",
        "    warmup_steps=500, # number of warmup steps for learning rate scheduler\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxpAnPp0LJLo"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_path = '/content/drive/MyDrive/IA/gpt2-contaAI-5/checkpoint-5000'\n",
        "conta_ai = pipeline('text-generation', model=model_path, tokenizer='pierreguillou/gpt2-small-portuguese', max_length=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ok6zUZjL3PM",
        "outputId": "e08907b2-4606-4ce7-a528-2e84710b695e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROMPT:\n",
            " Era uma casa velha e feia, com uma porta cheia de buracos e janelas remendadas. Emilia então decidiu entrar, ao girar \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SAIDA:\n",
            " Era uma casa velha e feia, com uma porta cheia de buracos e janelas remendadas. Emilia então decidiu entrar, ao girar em torno das janelas.\n",
            "     - Bem-vindo a casa.\n",
            "\n",
            "Eu nunca poderia ter imaginado que não houvesse um mundo com\n",
            "por parte de seus\n",
            "    -  É claro que Pantera?\n",
            "\n",
            "Os olhos de Matthew foram arregalados.\n",
            "O dia estava quente em seu corpo, e os olhos de Lucy brilharam ao redor da fogueira.\n",
            "\n",
            "     - Está vendo - disse Meredith, sorrindo. - O que eu acho que\n",
            "\n",
            "         Não sabia quando isso seria. O dia seria para ele, ou para Rosemary Rosemary.\n",
            "      Os olhos de Dana se fechavam quando ela puxou um para a frente e a observou atentamente. Em suas pálpebras e suas costas estavam se dobrando, o cabelo castanho escuro como as da noite.\n",
            "- Não me chamo papai - sussurrou.\n",
            "\n",
            "O que eu ia dizer? Se é que eu não ia, como ia? Eu sei lá, não era nada... A ideia que me incomodá-la. A ideia de um encontro com um garoto de quem já lhe dissera é para mim, um escândalo. Não vai a mim, não?\n",
            "Ora, você não tinha medo de perder as minhas forças quando cheguei a este momento! A mulher? Eu tinha vontade? Ele me enganou, e eu não ia? Como é que sabia ela, não estar a coisa alguma disso? Não, nem quero ter uma mãe!\n",
            "O coração? Ela era bonita e bonita. Ela poderia se perder as suas mulheres! Não é um, o que é, e não é uma coisa não. Você não! É uma mulher bonita, uma boa ideia! Não, não é algo inócua! Tudo aquilo!...\n",
            "Abro os olhos.\n",
            "               Ela tinha de se esconder. Se ele tivesse que correr, sim proporção a ela, o deixaria ir. O pior era que a perda das mãos de mim. Ela me queria me ser diferente, eu não perder da mãe e ficar, e se era.\n",
            "Fixando minhas mãos. - para ir e pra fora da sala para o carro como uma louca mulher como uma louca! A porta atrás de\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Era uma casa velha e feia, com uma porta cheia de buracos e janelas remendadas. Emilia então decidiu entrar, ao girar'\n",
        "print('PROMPT:\\n', prompt, '\\n\\n\\n\\n\\n')\n",
        "for i in range(1):\n",
        "  out = conta_ai(prompt)\n",
        "  prompt = out[0]['generated_text'] + '\\n'\n",
        "\n",
        "print('SAIDA:\\n', prompt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
